#!/usr/bin/env python3
"""ëª¨ë¸ë§: OLS â†’ Random Forest â†’ XGBoost â†’ SHAP ë¶„ì„"""
import os, warnings
warnings.filterwarnings('ignore')
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import json

DATA_DIR = os.path.join(os.path.dirname(__file__), 'data')
RESULTS_DIR = os.path.join(os.path.dirname(__file__), 'results')
os.makedirs(RESULTS_DIR, exist_ok=True)

def load_and_prepare():
    """ë°ì´í„° ë¡œë“œ ë° ëª¨ë¸ë§ìš© ë³€ìˆ˜ ì¤€ë¹„"""
    print("=" * 60)
    print("ë°ì´í„° ë¡œë“œ ë° ë³€ìˆ˜ ì¤€ë¹„")
    print("=" * 60)
    
    df = pd.read_csv(os.path.join(DATA_DIR, 'apartment_final.csv'))
    
    # êµ¬ ë‹¨ìœ„ ì‹œì„¤ ìˆ˜ ì§‘ê³„ ì¶”ê°€
    # í•™êµ
    schools = pd.read_csv(os.path.join(DATA_DIR, 'seoul_schools.csv'))
    if 'ë„ë¡œëª…ì£¼ì†Œ' in schools.columns:
        schools['êµ¬'] = schools['ë„ë¡œëª…ì£¼ì†Œ'].str.extract(r'ì„œìš¸íŠ¹ë³„ì‹œ\s+(\S+êµ¬)')
        school_count = schools.groupby('êµ¬').size().rename('í•™êµìˆ˜')
        # í•™êµ ì¢…ë¥˜ë³„
        if 'í•™êµì¢…ë¥˜' in schools.columns:
            elem = schools[schools['í•™êµì¢…ë¥˜'].str.contains('ì´ˆë“±', na=False)].groupby('êµ¬').size().rename('ì´ˆë“±í•™êµìˆ˜')
            middle = schools[schools['í•™êµì¢…ë¥˜'].str.contains('ì¤‘í•™', na=False)].groupby('êµ¬').size().rename('ì¤‘í•™êµìˆ˜')
            high = schools[schools['í•™êµì¢…ë¥˜'].str.contains('ê³ ë“±', na=False)].groupby('êµ¬').size().rename('ê³ ë“±í•™êµìˆ˜')
            school_detail = pd.concat([school_count, elem, middle, high], axis=1).fillna(0)
        else:
            school_detail = school_count.to_frame()
        df = df.merge(school_detail, on='êµ¬', how='left')
    
    # ì§€í•˜ì² ì—­ ìˆ˜ (êµ¬ ë‹¨ìœ„ â€” ì—­ ì´ë¦„ì—ì„œ ì¶”ì¶œ ì–´ë ¤ìš°ë¯€ë¡œ ì „ì²´ ì„œìš¸ í‰ê·  ì‚¬ìš©)
    subway = pd.read_csv(os.path.join(DATA_DIR, 'seoul_subway_stations.csv'))
    # ë…¸ì„  ìˆ˜ë¥¼ ë³€ìˆ˜ë¡œ í™œìš©
    
    # ê³µì› ìˆ˜ â€” êµ¬ ì •ë³´ ì¶”ì¶œ
    parks = pd.read_csv(os.path.join(DATA_DIR, 'seoul_parks.csv'))
    if any('ADDR' in c or 'ì£¼ì†Œ' in c or 'P_ADDR' in c for c in parks.columns):
        addr_col = [c for c in parks.columns if 'ADDR' in c.upper() or 'ì£¼ì†Œ' in c][0] if [c for c in parks.columns if 'ADDR' in c.upper() or 'ì£¼ì†Œ' in c] else None
        if addr_col:
            parks['êµ¬'] = parks[addr_col].str.extract(r'(\S+êµ¬)')
            park_count = parks.groupby('êµ¬').size().rename('ê³µì›ìˆ˜')
            df = df.merge(park_count, on='êµ¬', how='left')
    
    # ë°±í™”ì /ëŒ€í˜•ì í¬ ìˆ˜
    stores = pd.read_csv(os.path.join(DATA_DIR, 'seoul_large_stores.csv'))
    if 'RDNWHLADDR' in stores.columns:
        stores['êµ¬'] = stores['RDNWHLADDR'].str.extract(r'ì„œìš¸íŠ¹ë³„ì‹œ\s+(\S+êµ¬)')
        # ë°±í™”ì ë§Œ
        if 'UPTAENM' in stores.columns:
            dept_count = stores[stores['UPTAENM'] == 'ë°±í™”ì '].groupby('êµ¬').size().rename('ë°±í™”ì ìˆ˜')
            df = df.merge(dept_count, on='êµ¬', how='left')
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    fill_cols = ['í•™êµìˆ˜', 'ì´ˆë“±í•™êµìˆ˜', 'ì¤‘í•™êµìˆ˜', 'ê³ ë“±í•™êµìˆ˜', 'ê³µì›ìˆ˜', 'ë°±í™”ì ìˆ˜']
    for c in fill_cols:
        if c in df.columns:
            df[c] = df[c].fillna(0)
    
    # êµ¬ ê²°ì¸¡ ì œê±°
    df = df.dropna(subset=['êµ¬'])
    
    print(f"ìµœì¢… ë°ì´í„°: {len(df):,}ê±´")
    print(f"ì»¬ëŸ¼: {list(df.columns)}")
    
    return df

def select_features(df):
    """ëª¨ë¸ë§ìš© ë…ë¦½ë³€ìˆ˜ ì„ íƒ"""
    # ë…ë¦½ë³€ìˆ˜ í›„ë³´
    feature_candidates = [
        # ë¬¼ë¦¬ì  íŠ¹ì„±
        'ì „ìš©ë©´ì ', 'ì¸µ', 'ê±´ë¬¼ì—°ë ¹',
        # ì…ì§€/í™˜ê²½ (êµ¬ ë‹¨ìœ„)
        'ê°•ë‚¨êµ¬ë¶„', 'í•™êµìˆ˜', 'ì´ˆë“±í•™êµìˆ˜', 'ì¤‘í•™êµìˆ˜', 'ê³ ë“±í•™êµìˆ˜', 'ê³µì›ìˆ˜', 'ë°±í™”ì ìˆ˜',
        # ê±°ì‹œê²½ì œ
        'ê¸°ì¤€ê¸ˆë¦¬', 'CDê¸ˆë¦¬', 'ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜', 'M2',
    ]
    
    features = [f for f in feature_candidates if f in df.columns]
    target = 'ê±°ë˜ê¸ˆì•¡'
    
    # ê²°ì¸¡ì¹˜ ìˆëŠ” í–‰ ì œê±°
    subset = df[features + [target]].dropna()
    
    X = subset[features]
    y = subset[target]
    
    print(f"\në…ë¦½ë³€ìˆ˜ ({len(features)}ê°œ): {features}")
    print(f"ì¢…ì†ë³€ìˆ˜: {target}")
    print(f"ë¶„ì„ ë°ì´í„°: {len(subset):,}ê±´")
    
    return X, y, features

def evaluate_model(name, y_true, y_pred):
    """ëª¨ë¸ í‰ê°€ ì§€í‘œ"""
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    
    print(f"\n{'='*40}")
    print(f"ğŸ“Š {name} ê²°ê³¼")
    print(f"{'='*40}")
    print(f"  RÂ²:    {r2:.4f}")
    print(f"  RMSE:  {rmse:,.0f} (ë§Œì›)")
    print(f"  MAE:   {mae:,.0f} (ë§Œì›)")
    print(f"  MAPE:  {mape:.2f}%")
    
    return {'model': name, 'R2': round(r2, 4), 'RMSE': round(rmse, 0), 'MAE': round(mae, 0), 'MAPE': round(mape, 2)}

def main():
    # ë°ì´í„° ì¤€ë¹„
    df = load_and_prepare()
    X, y, features = select_features(df)
    
    # Train/Test Split (8:2)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    print(f"\nTrain: {len(X_train):,}ê±´ / Test: {len(X_test):,}ê±´")
    
    results = []
    
    # â”€â”€â”€ 1. OLS (ë‹¤ì¤‘íšŒê·€) â”€â”€â”€
    print("\n" + "â”" * 60)
    print("1ï¸âƒ£  OLS ë‹¤ì¤‘íšŒê·€ë¶„ì„")
    print("â”" * 60)
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    ols = LinearRegression()
    ols.fit(X_train_scaled, y_train)
    y_pred_ols = ols.predict(X_test_scaled)
    
    res_ols = evaluate_model("OLS ë‹¤ì¤‘íšŒê·€", y_test.values, y_pred_ols)
    results.append(res_ols)
    
    # OLS ê³„ìˆ˜
    print("\n  ğŸ“Œ OLS íšŒê·€ê³„ìˆ˜ (í‘œì¤€í™”):")
    coefs = pd.Series(ols.coef_, index=features).sort_values(key=abs, ascending=False)
    for feat, coef in coefs.items():
        print(f"    {feat:15s}: {coef:>10,.1f}")
    
    # â”€â”€â”€ 2. Random Forest â”€â”€â”€
    print("\n" + "â”" * 60)
    print("2ï¸âƒ£  Random Forest")
    print("â”" * 60)
    
    rf = RandomForestRegressor(n_estimators=200, max_depth=15, min_samples_leaf=10, 
                                n_jobs=-1, random_state=42)
    rf.fit(X_train, y_train)
    y_pred_rf = rf.predict(X_test)
    
    res_rf = evaluate_model("Random Forest", y_test.values, y_pred_rf)
    results.append(res_rf)
    
    # RF Feature Importance
    print("\n  ğŸ“Œ RF ë³€ìˆ˜ ì¤‘ìš”ë„:")
    fi_rf = pd.Series(rf.feature_importances_, index=features).sort_values(ascending=False)
    for feat, imp in fi_rf.items():
        bar = 'â–ˆ' * int(imp * 50)
        print(f"    {feat:15s}: {imp:.4f} {bar}")
    
    # â”€â”€â”€ 3. XGBoost â”€â”€â”€
    print("\n" + "â”" * 60)
    print("3ï¸âƒ£  XGBoost")
    print("â”" * 60)
    
    try:
        from xgboost import XGBRegressor
    except ImportError:
        print("  âš ï¸ xgboost ë¯¸ì„¤ì¹˜, ì„¤ì¹˜ ì¤‘...")
        os.system("pip install --break-system-packages xgboost -q")
        from xgboost import XGBRegressor
    
    xgb = XGBRegressor(
        n_estimators=500,
        max_depth=8,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        min_child_weight=5,
        reg_alpha=0.1,
        reg_lambda=1.0,
        random_state=42,
        n_jobs=-1,
        early_stopping_rounds=50,
    )
    xgb.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)
    y_pred_xgb = xgb.predict(X_test)
    
    res_xgb = evaluate_model("XGBoost", y_test.values, y_pred_xgb)
    results.append(res_xgb)
    
    # XGB Feature Importance
    print("\n  ğŸ“Œ XGBoost ë³€ìˆ˜ ì¤‘ìš”ë„:")
    fi_xgb = pd.Series(xgb.feature_importances_, index=features).sort_values(ascending=False)
    for feat, imp in fi_xgb.items():
        bar = 'â–ˆ' * int(imp * 50)
        print(f"    {feat:15s}: {imp:.4f} {bar}")
    
    # â”€â”€â”€ 4. SHAP ë¶„ì„ â”€â”€â”€
    print("\n" + "â”" * 60)
    print("4ï¸âƒ£  SHAP ë¶„ì„ (XGBoost)")
    print("â”" * 60)
    
    try:
        import shap
    except ImportError:
        print("  âš ï¸ shap ë¯¸ì„¤ì¹˜, ì„¤ì¹˜ ì¤‘...")
        os.system("pip install --break-system-packages shap -q")
        import shap
    
    explainer = shap.TreeExplainer(xgb)
    
    # ìƒ˜í”Œë§ (ì „ì²´ ë°ì´í„°ê°€ í¬ë©´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
    sample_size = min(5000, len(X_test))
    X_sample = X_test.sample(sample_size, random_state=42)
    shap_values = explainer.shap_values(X_sample)
    
    # SHAP í‰ê·  ì ˆëŒ€ê°’
    print(f"\n  ğŸ“Œ SHAP ë³€ìˆ˜ ì¤‘ìš”ë„ (í‰ê·  |SHAP|, ìƒ˜í”Œ {sample_size}ê±´):")
    shap_importance = pd.Series(np.abs(shap_values).mean(axis=0), index=features).sort_values(ascending=False)
    for feat, imp in shap_importance.items():
        bar = 'â–ˆ' * int(imp / shap_importance.max() * 30)
        print(f"    {feat:15s}: {imp:>10,.1f} {bar}")
    
    # SHAP ê²°ê³¼ ì €ì¥
    shap_df = pd.DataFrame(shap_values, columns=features)
    shap_df.to_csv(os.path.join(RESULTS_DIR, 'shap_values.csv'), index=False, encoding='utf-8-sig')
    
    # â”€â”€â”€ 5. ê°•ë‚¨ vs ë¹„ê°•ë‚¨ ë¹„êµ â”€â”€â”€
    print("\n" + "â”" * 60)
    print("5ï¸âƒ£  ê°•ë‚¨ vs ë¹„ê°•ë‚¨ ë¹„êµë¶„ì„")
    print("â”" * 60)
    
    for label, mask_val in [("ê°•ë‚¨3êµ¬ (ê°•ë‚¨/ì„œì´ˆ/ì†¡íŒŒ)", 1), ("ë¹„ê°•ë‚¨", 0)]:
        mask = X_test['ê°•ë‚¨êµ¬ë¶„'] == mask_val
        if mask.sum() == 0:
            continue
        y_sub = y_test[mask].values
        y_pred_sub = y_pred_xgb[mask]
        
        r2_sub = r2_score(y_sub, y_pred_sub)
        rmse_sub = np.sqrt(mean_squared_error(y_sub, y_pred_sub))
        print(f"\n  {label}:")
        print(f"    ê±´ìˆ˜: {mask.sum():,}")
        print(f"    í‰ê·  ì‹¤ê±°ë˜ê°€: {y_sub.mean():,.0f}ë§Œì›")
        print(f"    RÂ²: {r2_sub:.4f}")
        print(f"    RMSE: {rmse_sub:,.0f}ë§Œì›")
    
    # ê°•ë‚¨/ë¹„ê°•ë‚¨ SHAP ë¹„êµ
    gangnam_mask = X_sample['ê°•ë‚¨êµ¬ë¶„'] == 1
    if gangnam_mask.sum() > 0:
        print(f"\n  ğŸ“Œ SHAP ë³€ìˆ˜ ì¤‘ìš”ë„ ë¹„êµ:")
        print(f"  {'ë³€ìˆ˜':15s} | {'ê°•ë‚¨3êµ¬':>10s} | {'ë¹„ê°•ë‚¨':>10s} | {'ì°¨ì´':>10s}")
        print(f"  {'-'*15}-+-{'-'*10}-+-{'-'*10}-+-{'-'*10}")
        
        shap_gangnam = np.abs(shap_values[gangnam_mask.values]).mean(axis=0)
        shap_non = np.abs(shap_values[~gangnam_mask.values]).mean(axis=0)
        
        for i, feat in enumerate(features):
            diff = shap_gangnam[i] - shap_non[i]
            print(f"  {feat:15s} | {shap_gangnam[i]:>10,.1f} | {shap_non[i]:>10,.1f} | {diff:>+10,.1f}")
    
    # â”€â”€â”€ ê²°ê³¼ ì¢…í•© â”€â”€â”€
    print("\n" + "â”" * 60)
    print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì¢…í•©")
    print("â”" * 60)
    
    results_df = pd.DataFrame(results)
    print(results_df.to_string(index=False))
    
    results_df.to_csv(os.path.join(RESULTS_DIR, 'model_comparison.csv'), index=False, encoding='utf-8-sig')
    
    # ë³€ìˆ˜ ì¤‘ìš”ë„ ì €ì¥
    importance_df = pd.DataFrame({
        'variable': features,
        'OLS_coef': [coefs.get(f, 0) for f in features],
        'RF_importance': [fi_rf.get(f, 0) for f in features],
        'XGB_importance': [fi_xgb.get(f, 0) for f in features],
        'SHAP_importance': [shap_importance.get(f, 0) for f in features],
    }).sort_values('SHAP_importance', ascending=False)
    importance_df.to_csv(os.path.join(RESULTS_DIR, 'feature_importance.csv'), index=False, encoding='utf-8-sig')
    
    print(f"\nâœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
    print(f"  - {RESULTS_DIR}/model_comparison.csv")
    print(f"  - {RESULTS_DIR}/feature_importance.csv")
    print(f"  - {RESULTS_DIR}/shap_values.csv")

if __name__ == '__main__':
    main()
